{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from os.path import join, isdir, exists\n",
    "from os import listdir, makedirs\n",
    "from utils import *\n",
    "from SSNN import SSNN\n",
    "import time\n",
    "from object_boundaries import generate_bounding_boxes\n",
    "import os\n",
    "import psutil\n",
    "from compute_mAP3 import compute_mAP\n",
    "from compute_bbox_accuracy import compute_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Tensorflow flags boilerplate code.\n",
    "flags = tf.app.flags\n",
    "FLAGS = flags.FLAGS\n",
    "\n",
    "#########\n",
    "# FLAGS #\n",
    "#########\n",
    "\n",
    "# Data information: loading and saving options.\n",
    "flags.DEFINE_string('data_dir', '/home/ryan/cs/datasets/SSNN/buildings', 'Path to base directory.')\n",
    "flags.DEFINE_string('dataset_name', 'stanford', 'Name of dataset. Supported datasets are [stanford, matterport].')\n",
    "flags.DEFINE_bool('load_from_npy', False, 'Whether to load from preloaded dataset')\n",
    "flags.DEFINE_bool('load_probe_output', False, 'Load the probe output if a valid file exists.')\n",
    "flags.DEFINE_integer('rotated_copies', 0, 'Number of times the dataset is copied and rotated for data augmentation.')\n",
    "flags.DEFINE_string('checkpoint_save_dir', None, 'Path to saving checkpoint.')\n",
    "flags.DEFINE_string('checkpoint_load_dir', None, 'Path to loading checkpoint.')\n",
    "flags.DEFINE_string('checkpoint_load_iter', 50, 'Iteration from save dir to load.')\n",
    "flags.DEFINE_float('checkpoint_save_interval', 10, 'If checkpoint_save_interval is defined, then sets save interval.')\n",
    "flags.DEFINE_boolean('use_rgb', True, 'If True, then loads colored pointclouds. Else, loads uncolored pointclouds.')\n",
    "flags.DEFINE_string('single_class', None, 'Class name for single object detector.')\n",
    "\n",
    "# Training hyperparameters.\n",
    "flags.DEFINE_integer('num_epochs', 100, 'Number of epochs to train.')\n",
    "flags.DEFINE_float('test_split', 0.1, 'Percentage of input data to use as test data.')\n",
    "flags.DEFINE_float('val_split', 0.1, 'Percentage of input data to use as validation. Taken after the test split.')\n",
    "flags.DEFINE_float('learning_rate', 0.00005, 'Learning rate for training.')\n",
    "flags.DEFINE_float('loc_loss_lambda', 1, 'Relative weight of localization params.')\n",
    "flags.DEFINE_float('dropout', 0.5, 'Keep probability for layers with dropout.')\n",
    "\n",
    "# Probing hyperparameters.\n",
    "flags.DEFINE_integer('num_steps', 32, 'Number of intervals to sample from in each xyz direction.')\n",
    "flags.DEFINE_integer('k_size_factor', 3, 'Size of the probing kernel with respect to the step size.')\n",
    "flags.DEFINE_integer('batch_size', 8, 'Batch size for training.')\n",
    "flags.DEFINE_integer('num_kernels', 8, 'Number of kernels to probe with.')\n",
    "flags.DEFINE_integer('probes_per_kernel', 64, 'Number of sample points each kernel has.')\n",
    "flags.DEFINE_integer('num_dot_layers', 16, 'Number of dot product layers per kernel')\n",
    "\n",
    "NUM_SCALES = 3\n",
    "NUM_HOOK_STEPS = int(FLAGS.num_steps / 2)\n",
    "DIMS = np.array([7.5, 7.5, 7.5])\n",
    "\n",
    "# Define sets for training and testing (Stanford dataset)\n",
    "TRAIN_AREAS = ['Area_1', 'Area_2', 'Area_3', 'Area_4', 'Area_5'] \n",
    "TEST_AREAS = ['Area_6']\n",
    "\n",
    "# Define categories.\n",
    "# CATEGORIES = ['box', 'picture', 'pillow', 'curtain', 'table', 'bench', 'side table', 'window', 'bed', 'tv', \n",
    "#                   'heater', 'pot', 'bottles', 'washbasin', 'light', 'clothes', 'bin', 'cabinet', 'radiator', 'bookcase',\n",
    "#                   'button', 'toilet paper', 'toilet', 'control panel', 'towel']\n",
    "\n",
    "\n",
    "CATEGORIES = ['sofa', 'table', 'chair', 'board']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constant paths (TODO: make this more organized between datasets)\n",
    "intermediate_dir = join(FLAGS.data_dir, 'intermediates')\n",
    "if not exists(intermediate_dir):\n",
    "  makedirs(intermediate_dir)\n",
    "output_dir = join(FLAGS.data_dir, 'outputs')\n",
    "if not exists(output_dir):\n",
    "  makedirs(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Raw inputs\n",
    "X_TRN            = join(intermediate_dir, 'trn_data.npy')\n",
    "YS_TRN           = join(intermediate_dir, 'trn_seg_labels.npy')\n",
    "YL_TRN           = join(intermediate_dir, 'trn_cls_labels.npy')\n",
    "PROBE_TRN        = join(intermediate_dir, 'trn_probe_out.npy') # memmap\n",
    "\n",
    "X_TEST           = join(intermediate_dir, 'test_data.npy')\n",
    "YS_TEST          = join(intermediate_dir, 'test_seg_labels.npy')\n",
    "YL_TEST          = join(intermediate_dir, 'test_cls_labels.npy')\n",
    "PROBE_TEST       = join(intermediate_dir, 'test_probe_out.npy') # memmap\n",
    "\n",
    "# Processed inputs and ouputs\n",
    "CLS_TRN_LABELS   = join(output_dir, 'cls_trn_labels.npy')\n",
    "LOC_TRN_LABELS   = join(output_dir, 'loc_trn_labels.npy')\n",
    "BBOX_TRN_LABELS  = join(output_dir, 'bbox_trn_labels.npy')\n",
    "CLS_TRN_BBOX     = join(output_dir, 'bbox_trn_cls_labels.npy')\n",
    "\n",
    "CLS_TEST_LABELS  = join(output_dir, 'cls_test_labels.npy')\n",
    "LOC_TEST_LABELS  = join(output_dir, 'loc_test_labels.npy')\n",
    "BBOX_TEST_LABELS = join(output_dir, 'bbox_test_labels.npy')\n",
    "CLS_TEST_BBOX    = join(output_dir, 'bbox_test_cls_labels.npy')\n",
    "\n",
    "CLS_PREDS        = join(output_dir, 'cls_predictions.npy')\n",
    "LOC_PREDS        = join(output_dir, 'loc_predictions.npy')\n",
    "BBOX_PREDS       = join(output_dir, 'bbox_predictions.npy')\n",
    "BBOX_CLS_PREDS   = join(output_dir, 'bbox_cls_predictions.npy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_input(model, data_dir, areas, x_path, ys_path, yl_path, probe_path, \n",
    "                      cls_labels, loc_labels, bbox_labels, cls_by_box, load_from_npy, load_probe_output, num_copies=0, is_train=True, oh_mapping=None):\n",
    "  \"\"\"\n",
    "  Converts raw data into form that can be fed into the ML pipeline. Operations include normalization, augmentation, \n",
    "  label ggeneration, and probing.\n",
    "  \"\"\"\n",
    "\n",
    "  input_type = \"train\" if is_train else \"test\"\n",
    "  assert FLAGS.dataset_name in ['stanford', 'matterport'], 'Supported datasets are stanford and matterport.'\n",
    "\n",
    "  print(\"Running pre-processing for {} set.\".format(input_type))\n",
    "  if FLAGS.dataset_name == 'stanford':\n",
    "    normalize_pointclouds_fn = normalize_pointclouds_stanford\n",
    "\n",
    "  elif FLAGS.dataset_name == 'matterport':\n",
    "    normalize_pointclouds_fn = normalize_pointclouds_matterport\n",
    "\n",
    "  if FLAGS.dataset_name == 'matterport':\n",
    "    X_raw, yb_raw, yl, new_ds = load_points_matterport(path=data_dir, X_npy_path=x_path,\n",
    "                                    yb_npy_path = ys_path, yl_npy_path = yl_path, \n",
    "                                    load_from_npy=load_from_npy, is_train=is_train,\n",
    "                                    categories=CATEGORIES, train_test_split=1.0 - FLAGS.test_split, use_rgb=FLAGS.use_rgb)\n",
    "  elif FLAGS.dataset_name == 'stanford':\n",
    "    X_raw, yb_raw, yl, new_ds = load_points_stanford(path=data_dir, X_npy_path=x_path,\n",
    "                                  ys_npy_path = ys_path, yl_npy_path = yl_path, \n",
    "                                  load_from_npy=load_from_npy, areas=areas, categories=CATEGORIES)\n",
    "\n",
    "  print(\"\\tLoaded {} pointclouds for {}.\".format(len(X_raw), input_type))\n",
    "  process = psutil.Process(os.getpid())\n",
    " \n",
    "  # Shift to the same coordinate space between pointclouds while getting the max\n",
    "  # width, height, and depth dims of all rooms.\n",
    "\n",
    "  print(\"\\tNormalizing pointclouds...\")\n",
    "  X_cont, dims, ys = normalize_pointclouds_fn(X_raw, yb_raw, DIMS)\n",
    "\n",
    "  #print(\"Rotating dataset...\")\n",
    "  #X_cont, ys, yl = rotate_pointclouds(X_cont, ys, list(yl), num_rotations=num_copies)\n",
    "\n",
    "  yl = np.array(yl)\n",
    "  kernel_size = DIMS / NUM_HOOK_STEPS\n",
    "\n",
    "  if FLAGS.dataset_name == 'stanford':\n",
    "    print(\"\\tGenerating bboxes...\")\n",
    "    bboxes = generate_bounding_boxes(ys, bbox_labels)\n",
    "  elif FLAGS.dataset_name == 'matterport':\n",
    "    bboxes = ys\n",
    "  np.save(bbox_labels, bboxes)\n",
    "\n",
    "  print(\"\\tProcessing labels...\")\n",
    "  y_cat_one_hot, mapping = one_hot_vectorize_categories(yl, mapping=oh_mapping)\n",
    "  np.save(cls_by_box, y_cat_one_hot)\n",
    "  y_cls, y_loc = create_jaccard_labels(bboxes, y_cat_one_hot, len(mapping)+1, NUM_HOOK_STEPS, kernel_size)\n",
    "\n",
    "  np.save(cls_labels, y_cls)\n",
    "  np.save(loc_labels, y_loc)\n",
    "\n",
    "  # Probe processing.\n",
    "  if exists(probe_path) and load_probe_output and not new_ds:\n",
    "    # Used for developing so redudant calculations are omitted.\n",
    "    print (\"\\tLoading previous probe output...\")\n",
    "    # X = np.load(probe_path)\n",
    "    X = np.memmap(probe_path, dtype='float32', mode='r', shape=(len(X_cont), FLAGS.num_steps, \n",
    "                             FLAGS.num_steps, FLAGS.num_steps, FLAGS.num_kernels, FLAGS.probes_per_kernel, 4))\n",
    "  else:\n",
    "    print(\"\\tAmount of memory used before probing: {}GB\".format(process.memory_info().rss // 1e9))\n",
    "    print(\"\\tRunning probe operation...\")\n",
    "    probe_start = time.time()\n",
    "    probe_shape = (len(X_cont), NUM_HOOK_STEPS, NUM_HOOK_STEPS, NUM_HOOK_STEPS, FLAGS.num_kernels, FLAGS.probes_per_kernel)\n",
    "    X, problem_pcs = model.probe(X_cont, probe_shape, probe_path)\n",
    "    probe_time = time.time() - probe_start\n",
    "    print(\"\\tProbe operation took {:.4f} seconds to run.\".format(probe_time))\n",
    "    print(\"\\tAmount of memory used after probing: {}GB\".format(process.memory_info().rss // 1e9))\n",
    "    \n",
    "    # TODO: delete hard-coded elements of problem pointcloud removal (see SSNN.py counter var if/else logic).\n",
    "    for problem_pc in problem_pcs:\n",
    "      y_cls[problem_pc] = y_cls[problem_pc-1]\n",
    "      y_loc[problem_pc] = y_loc[problem_pc-1]\n",
    "\n",
    "  print(\"\\tFinished pre-processing of {} set.\".format(input_type))\n",
    "  return X, y_cls, y_loc, y_cat_one_hot, bboxes, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probe op with 8 kernels and 64 probes per kernel.\n",
      "Initialized new SSNN model.\n",
      "Running pre-processing for train set.\n",
      "\tLoading points from directory...\n",
      "Loading area Area_1...\n",
      "\tLoading room WC_1...\n",
      "\tLoading room conferenceRoom_1...\n",
      "\tLoading room conferenceRoom_2...\n",
      "\tLoading room copyRoom_1...\n",
      "\tLoading room hallway_1...\n",
      "\tLoading room hallway_2...\n",
      "\tLoading room hallway_3...\n",
      "\tLoading room hallway_4...\n",
      "\tLoading room hallway_5...\n",
      "\tLoading room hallway_6...\n",
      "\tLoading room hallway_7...\n",
      "\tLoading room hallway_8...\n",
      "\tLoading room office_1...\n",
      "\tLoading room office_10...\n",
      "\tLoading room office_11...\n",
      "\tLoading room office_12...\n",
      "\tLoading room office_13...\n",
      "\tLoading room office_14...\n",
      "\tLoading room office_15...\n",
      "\tLoading room office_16...\n",
      "\tLoading room office_17...\n",
      "\tLoading room office_18...\n",
      "\tLoading room office_19...\n",
      "\tLoading room office_2...\n",
      "\tLoading room office_20...\n",
      "\tLoading room office_21...\n",
      "\tLoading room office_22...\n",
      "\tLoading room office_23...\n",
      "\tLoading room office_24...\n",
      "\tLoading room office_25...\n",
      "\tLoading room office_26...\n",
      "\tLoading room office_27...\n"
     ]
    }
   ],
   "source": [
    "kernel_size = DIMS / FLAGS.num_steps\n",
    "\n",
    "# Initialize model. max_room_dims and step_size are in meters.\n",
    "ssnn = SSNN(DIMS, num_kernels=FLAGS.num_kernels, \n",
    "                probes_per_kernel=FLAGS.probes_per_kernel, \n",
    "                probe_steps=FLAGS.num_steps, probe_hook_steps=NUM_HOOK_STEPS,\n",
    "                num_scales=NUM_SCALES,\n",
    "                dot_layers=FLAGS.num_dot_layers,\n",
    "                ckpt_save=FLAGS.checkpoint_save_dir,\n",
    "                ckpt_load=FLAGS.checkpoint_load_dir,\n",
    "                ckpt_load_iter=FLAGS.checkpoint_load_iter,\n",
    "                loc_loss_lambda=FLAGS.loc_loss_lambda,\n",
    "                learning_rate=FLAGS.learning_rate,\n",
    "                dropout=FLAGS.dropout,\n",
    "                k_size_factor=FLAGS.k_size_factor,\n",
    "                num_classes=len(CATEGORIES)+1)\n",
    "\n",
    "\n",
    "load_probe = FLAGS.load_probe_output and FLAGS.load_from_npy\n",
    "\n",
    "# Pre-process train data. Train/test data pre-processing is split for easier data streaming.\n",
    "X, y_cls, y_loc, y_cat_one_hot, bboxes, mapping = preprocess_input(ssnn, FLAGS.data_dir, TRAIN_AREAS, X_TRN, YS_TRN, YL_TRN, PROBE_TRN, \n",
    "                  CLS_TRN_LABELS, LOC_TRN_LABELS, BBOX_TRN_LABELS, CLS_TRN_BBOX, FLAGS.load_from_npy,\n",
    "                  load_probe, num_copies=FLAGS.rotated_copies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.\n",
    "train_split = int((FLAGS.val_split) * X.shape[0])\n",
    "X_trn = X[train_split:]\n",
    "y_trn_cls = y_cls[train_split:]\n",
    "y_trn_loc = y_loc[train_split:]\n",
    "y_trn_one_hot = y_cat_one_hot[train_split:]\n",
    "trn_bboxes = bboxes[train_split:]\n",
    "np.save('y_cls.npy', y_trn_cls)\n",
    "X_val = X[:train_split]\n",
    "y_val_cls = y_cls[:train_split]\n",
    "y_val_loc = y_loc[:train_split]\n",
    "y_val_one_hot = y_cat_one_hot[:train_split]\n",
    "val_bboxes = bboxes[:train_split]\n",
    "print(\"Beginning training...\")\n",
    "ssnn.train_val(X_trn, y_trn_cls, y_trn_loc, X_val, y_val_cls, y_val_loc, val_bboxes, y_val_one_hot, epochs=FLAGS.num_epochs, batch_size=FLAGS.batch_size, save_interval=FLAGS.checkpoint_save_interval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, _, _, _, _, _ = preprocess_input(ssnn, FLAGS.data_dir, TEST_AREAS, X_TEST, YS_TEST, YL_TEST, PROBE_TEST, \n",
    "                      CLS_TEST_LABELS, LOC_TEST_LABELS, BBOX_TEST_LABELS, CLS_TEST_BBOX, FLAGS.load_from_npy,\n",
    "                      load_probe, is_train=False, oh_mapping=mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test model. Using validation since we won't be using real \n",
    "# \"test\" data yet. Preds will be an array of bounding boxes. \n",
    "start_test = time.time()\n",
    "# cls_preds, loc_preds = ssnn.test(X_test)\n",
    "cls_preds, loc_preds = ssnn.test(X_test)\n",
    "end_test = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Time to run {} test samples took {} seconds.\".format(X_test.shape[0], end_test-start_test))\n",
    "\n",
    "# Save output.\n",
    "save_output(CLS_PREDS, LOC_PREDS, cls_preds, loc_preds, \n",
    "                         NUM_HOOK_STEPS, NUM_SCALES, len(CATEGORIES)+1)\n",
    "\n",
    "cls_f = np.load(CLS_PREDS)\n",
    "loc_f = np.load(LOC_PREDS)\n",
    "\n",
    "bboxes, bboxes_cls = output_to_bboxes(cls_f, loc_f, NUM_HOOK_STEPS, NUM_SCALES, \n",
    "                        DIMS/NUM_HOOK_STEPS, BBOX_PREDS, BBOX_CLS_PREDS, conf_threshold=0.1)\n",
    "\n",
    "\n",
    "\n",
    "# Compute recall and precision.\n",
    "compute_mAP(bboxes, bboxes_cls, np.load(BBOX_TEST_LABELS), np.load(CLS_TEST_BBOX), mapping=mapping, threshold=0.25)\n",
    "compute_mAP(bboxes, bboxes_cls, np.load(BBOX_TEST_LABELS), np.load(CLS_TEST_BBOX), mapping=mapping, threshold=0.5, plot_category=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
